{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from csv files to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=spark.read.csv('C:/Manidhar/MachineLearningLab/datasets/titanic/train.csv',header=True)\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|Pclass|Survived|\n",
      "+------+--------+\n",
      "|     3|       0|\n",
      "|     1|       1|\n",
      "|     3|       1|\n",
      "|     1|       1|\n",
      "|     3|       0|\n",
      "+------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df2.select('Pclass','Survived')\n",
    "df3.show(n=5)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pclass: double (nullable = true)\n",
      " |-- Survived: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df3.select(df3.Pclass.cast('double'),df3.Survived.cast('double'))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+\n",
      "|Pclass|Survived|Features|\n",
      "+------+--------+--------+\n",
      "|   3.0|     0.0|   [3.0]|\n",
      "|   1.0|     1.0|   [1.0]|\n",
      "|   3.0|     1.0|   [3.0]|\n",
      "|   1.0|     1.0|   [1.0]|\n",
      "|   3.0|     0.0|   [3.0]|\n",
      "+------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=VectorAssembler(inputCols=['Pclass'],outputCol='Features').transform(df3)\n",
    "df3.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1=DecisionTreeClassifier(featuresCol='Features',labelCol='Survived',maxDepth=10,impurity='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12=dt1.fit(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model12.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model12.numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4bb78b240756359818f6) of depth 2 with 5 nodes\n",
      "  If (feature 0 <= 2.5)\n",
      "   If (feature 0 <= 1.5)\n",
      "    Predict: 1.0\n",
      "   Else (feature 0 > 1.5)\n",
      "    Predict: 0.0\n",
      "  Else (feature 0 > 2.5)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model12.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4=spark.read.csv('C:/Manidhar/MachineLearningLab/datasets/titanic/test.csv',header=True)\n",
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pclass: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=df4.select('Pclass')\n",
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pclass: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=df5.select(df5.Pclass.cast('double'))\n",
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|Pclass|Features|\n",
      "+------+--------+\n",
      "|   3.0|   [3.0]|\n",
      "|   3.0|   [3.0]|\n",
      "|   2.0|   [2.0]|\n",
      "|   3.0|   [3.0]|\n",
      "|   3.0|   [3.0]|\n",
      "+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=VectorAssembler(inputCols=['Pclass'],outputCol='Features').transform(df5)\n",
    "df5.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+--------------------+----------+\n",
      "|Pclass|Features|rawPrediction|         probability|prediction|\n",
      "+------+--------+-------------+--------------------+----------+\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   2.0|   [2.0]|  [97.0,87.0]|[0.52717391304347...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   2.0|   [2.0]|  [97.0,87.0]|[0.52717391304347...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   3.0|   [3.0]|[372.0,119.0]|[0.75763747454175...|       0.0|\n",
      "|   1.0|   [1.0]| [80.0,136.0]|[0.37037037037037...|       1.0|\n",
      "|   1.0|   [1.0]| [80.0,136.0]|[0.37037037037037...|       1.0|\n",
      "|   2.0|   [2.0]|  [97.0,87.0]|[0.52717391304347...|       0.0|\n",
      "|   1.0|   [1.0]| [80.0,136.0]|[0.37037037037037...|       1.0|\n",
      "+------+--------+-------------+--------------------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6=model12.transform(df5)\n",
    "df6.show(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.coalesce(1).select('prediction').write.csv('test_predict1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
